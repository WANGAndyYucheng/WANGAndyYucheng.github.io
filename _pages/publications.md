---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


<style>
  .pub-card {
    margin: 1.25rem 0 1.75rem 0;
  }
  .pub-teaser {
    float: right;
    width: 400px;
    max-width: 40%;
    margin-right: 0;
    margin-top: 0.25rem;
    margin-left: 1rem;
    margin-bottom: 0.5rem;
    border: 1px solid #e6e6e6;
  }
  .pub-card-text {
    flex: 1;
    min-width: 240px;
  }
  .pub-card-text p {
    margin: 0.15rem 0 0.55rem 0;
  }
  .pub-title-row {
    margin: 0.15rem 0 0.55rem 0;
  }
  .pub-title-row .pub-venue {
    margin-bottom: 0.45rem;
  }
  .pub-title-row strong {
    display: block;
  }
  .pub-venue {
    display: inline-block;
    font-size: 0.85rem;
    background: #0c4da2;
    color: #fff;
    padding: 0.12rem 0.45rem;
    border-radius: 2px;
  }
  @media (max-width: 760px) {
    .pub-teaser {
      float: none;
      display: block;
      max-width: 100%;
      width: 100%;
      margin-left: 0;
      margin-bottom: 0.8rem;
    }
  }
</style>

<div class="pub-card">
  <div class="pub-card-text">
    <img class="pub-teaser" src="/images/pubs/CVGAS.png" alt="Controllable Video Generation teaser">
    <p class="pub-title-row">
      <span class="pub-venue">arXiv 2025</span>
      <strong>Controllable Video Generation: A Survey</strong>
    </p>
    <p>Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, <strong>Yucheng WANG</strong>, Mingzhe Zheng, Bingyuan Wang, Qinghe Wang, Xuanhua He, Hongfa Wang, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Sirui Han, Yike Guo, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen</p>
    <p>
      <a href="https://arxiv.org/abs/2507.16869"><strong>Paper</strong></a>
      /
      <a href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation"><strong>Code</strong></a>
    </p>
  </div>
</div>

<div class="pub-card">
  <div class="pub-card-text">
    <video class="pub-teaser" autoplay loop muted playsinline controls>
      <source src="/images/pubs/copart.mp4" type="video/mp4">
    </video>
    <p class="pub-title-row">
      <span class="pub-venue">ICCV 2025</span>
      <strong>From One to More: Contextual Part Latents for 3D Generation</strong>
    </p>
    <p>Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, <strong>Yucheng WANG</strong>, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu</p>
    <p>
      <a href="https://arxiv.org/abs/2507.08772"><strong>Paper</strong></a>
      /
      <a href="https://hkdsc.github.io/project/copart/"><strong>Project</strong></a>
      /
      <a href="https://github.com/hkdsc/copart"><strong>Code</strong></a>
      /
      <a href="https://huggingface.co/datasets/dscdyc/partverse"><strong>Dataset</strong></a>
    </p>
  </div>
</div>

<div class="pub-card">
  <div class="pub-card-text">
    <img class="pub-teaser" src="/images/pubs/MoDiT.png" alt="MoDiT teaser">
    <p class="pub-title-row">
      <span class="pub-venue">Thesis</span>
      <strong>MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation</strong>
    </p>
    <p><strong>Yucheng WANG</strong>, Dan Xu</p>
    <p>
      <a href="https://arxiv.org/abs/2507.05092"><strong>Paper</strong></a>
    </p>
  </div>
</div>